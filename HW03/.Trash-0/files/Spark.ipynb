{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088dffc4-779e-4d3a-9524-da05a00e764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Heart Disease Prediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv('data/heart_disease.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d3e38-0ce1-4afb-b4eb-5b770793a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d79b54-a0eb-46c8-8b70-87f4e8a55465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to retain\n",
    "columns_to_keep = [\n",
    "    'age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs', \n",
    "    'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', \n",
    "    'oldpeak', 'slope', 'target'\n",
    "]\n",
    "\n",
    "# Select the columns to retain\n",
    "df = df.select(columns_to_keep)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02a11a-f414-46f8-9143-b262d26a6030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col as spark_col, when\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Define the columns explicitly\n",
    "binary_categorical_cols = ['sex', 'painloc', 'painexer', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'exang', 'slope', 'target']\n",
    "numerical_cols = ['age', 'thaldur', 'trestbps', 'oldpeak', 'thalach']\n",
    "\n",
    "# Cleaning 'trestbps': Replace values less than 100 mm Hg with the median of values >= 100\n",
    "median_trestbps = df.filter(spark_col('trestbps') >= 100).approxQuantile('trestbps', [0.5], 0.001)[0]\n",
    "df = df.withColumn('trestbps', when(spark_col('trestbps') < 100, median_trestbps).otherwise(spark_col('trestbps')))\n",
    "\n",
    "# Cleaning 'oldpeak': Replace values less than 0 and those greater than 4 with the median of values within range\n",
    "median_oldpeak = df.filter((spark_col('oldpeak') >= 0) & (spark_col('oldpeak') <= 4)).approxQuantile('oldpeak', [0.5], 0.001)[0]\n",
    "df = df.withColumn('oldpeak', when((spark_col('oldpeak') < 0) | (spark_col('oldpeak') > 4), median_oldpeak).otherwise(spark_col('oldpeak')))\n",
    "\n",
    "# Impute remaining numerical columns using median\n",
    "imputer = Imputer(inputCols=numerical_cols, outputCols=[f\"{col}_imputed\" for col in numerical_cols]).setStrategy(\"median\")\n",
    "df_imputed = imputer.fit(df).transform(df)\n",
    "\n",
    "# Drop the original columns and rename imputed columns\n",
    "for num_col in numerical_cols:\n",
    "    df_imputed = df_imputed.drop(num_col).withColumnRenamed(f\"{num_col}_imputed\", num_col)\n",
    "\n",
    "# Ensure valid binary values (0 or 1) where applicable for specific columns\n",
    "for binary_col in ['fbs', 'prop', 'nitr', 'pro', 'diuretic']:\n",
    "    df_imputed = df_imputed.withColumn(binary_col, when(spark_col(binary_col) > 1, None).otherwise(spark_col(binary_col)))\n",
    "\n",
    "# Note: KNN imputation and classification should be handled differently in Spark; using Spark's MLlib for similar functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec784cd2-8347-4899-9227-0832bb3f89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Function to get the mode of a column\n",
    "def get_mode(df, col_name):\n",
    "    return df.groupBy(col_name).count().orderBy(F.desc('count')).first()[0]\n",
    "\n",
    "# Fill missing values in categorical columns with the mode\n",
    "for col in binary_categorical_cols:\n",
    "    mode_value = get_mode(df_imputed, col)\n",
    "    df_imputed = df_imputed.fillna({col: mode_value})\n",
    "\n",
    "# Verify if there are any missing values left\n",
    "for col in binary_categorical_cols:\n",
    "    print(f\"Missing values in {col}: {df_imputed.filter(F.col(col).isNull()).count()}\")\n",
    "\n",
    "# Drop rows where the slope value is 0\n",
    "df_imputed = df_imputed.filter(df_imputed.slope != 0)\n",
    "\n",
    "# Verify if there are any slope values of 0 left\n",
    "print(f\"Remaining rows with slope value 0: {df_imputed.filter(F.col('slope') == 0).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbef04-2f1c-4cf6-a9bb-7e94ea95b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify binary and categorical columns\n",
    "binary_categorical_cols = ['sex', 'painloc', 'painexer', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'exang', 'slope', 'target']\n",
    "\n",
    "# Display value counts for each binary or categorical column\n",
    "for column in binary_categorical_cols:\n",
    "    print(f\"\\nValue Counts for {column}:\")\n",
    "    df_imputed.groupBy(column).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb559dd-d636-4a8c-8950-31c8e4eaffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['age', 'thaldur', 'trestbps', 'oldpeak', 'thalach']\n",
    "\n",
    "# Display basic statistics for numerical columns\n",
    "print(\"Basic Statistics for Numerical Columns:\")\n",
    "df_imputed.select(numerical_cols).describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f6242-b07f-434f-9986-e1a266a29af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure the notebook to show plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Optional: Configure the style for seaborn plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Convert the DataFrame to Pandas for plotting\n",
    "numerical_df = df_imputed.select(numerical_cols).toPandas()\n",
    "\n",
    "# Creating a combined box plot\n",
    "plt.figure(figsize=(12, 6))  # Set the figure size\n",
    "sns.boxplot(data=numerical_df, orient=\"v\")  # Vertical box plots\n",
    "plt.title('Combined Box Plot for Numerical Columns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8eb6dc-95f3-4263-abe1-9b8bc6ea8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to Pandas for plotting\n",
    "binary_df = df_imputed.select(binary_categorical_cols).toPandas()\n",
    "\n",
    "# Determine the layout size\n",
    "n_cols = 4  # Increase number of columns in the grid for a more compact display\n",
    "n_rows = (len(binary_categorical_cols) + n_cols - 1) // n_cols  # Calculate required number of rows in the grid\n",
    "\n",
    "# Create figure and axes for the subplots\n",
    "plt.figure(figsize=(12, 10))  # Adjust overall figure size to reduce plot width\n",
    "\n",
    "for i, col in enumerate(binary_categorical_cols):\n",
    "    plt.subplot(n_rows, n_cols, i + 1)  # Create a subplot for each column\n",
    "    sns.countplot(x=binary_df[col], color='skyblue')  # Specify the color here\n",
    "    plt.title(f'Value Counts for {col}')\n",
    "    plt.xticks(rotation=0)  # Rotate x labels for better readability if necessary\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)  # Adjust the space between plots\n",
    "\n",
    "# Add a central title for the entire figure\n",
    "plt.suptitle('Distribution of Binary and Categorical Variables', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()  # Adjust subplots to fit into the figure area nicely\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09feedf6-6d56-48c9-86cb-e03ea4081841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# URL of the page to be scraped\n",
    "url = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Attempt to find the table by a unique characteristic such as a caption\n",
    "    caption_text = \"Proportion of people 15 years and over who were current daily smokers by age, 2011â€“12 to 2022\"\n",
    "    table = soup.find(\"caption\", string=lambda text: caption_text in text if text else False).find_parent('table') if soup.find(\"caption\", string=lambda text: caption_text in text if text else False) else None\n",
    "\n",
    "    if table:\n",
    "        # Extracting headers from the table's header row\n",
    "        headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')[1:]]  # Skip the first empty <th>\n",
    "        headers.insert(0, \"Age Group\")  # Manually add \"Age Group\" as the first header\n",
    "\n",
    "        # Collecting data rows\n",
    "        data = []\n",
    "        for row in table.find('tbody').find_all('tr'):\n",
    "            cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            age_group = row.find('th').get_text(strip=True) if row.find('th') else \"\"\n",
    "            cells.insert(0, age_group)  # Insert the age group at the start of the list of cells\n",
    "\n",
    "            if len(cells) == len(headers):  # Ensure the row has the correct number of elements\n",
    "                data.append(cells)\n",
    "\n",
    "        # Creating a Spark DataFrame\n",
    "        df_age_group = spark.createDataFrame(data, schema=headers)\n",
    "\n",
    "        # Convert relevant columns to float\n",
    "        for col_name in ['2011â€“12 (%)', '2014â€“15 (%)', '2017â€“18 (%)', '2022 (%)']:\n",
    "            df_age_group = df_age_group.withColumn(col_name, col(col_name).cast('float'))\n",
    "\n",
    "        # Compute the average smoking rate across all specified years for each row\n",
    "        df_age_group = df_age_group.withColumn(\n",
    "            'Average Smoking Rate (%)',\n",
    "            (col('2011â€“12 (%)') + col('2014â€“15 (%)') + col('2017â€“18 (%)') + col('2022 (%)')) / 4\n",
    "        )\n",
    "\n",
    "        # Select only the age group and the average smoking rate\n",
    "        df_age_group = df_age_group.select('Age Group', 'Average Smoking Rate (%)')\n",
    "\n",
    "        df_age_group.show(8)\n",
    "    else:\n",
    "        print(\"Table not found.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve data, status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309a00c-2846-4b28-89e9-334e067648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# URL of the page to be scraped\n",
    "url = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Attempt to find the table by searching for a specific caption\n",
    "    caption_text = \"Proportion of people 15 years and over who were current daily smokers by age and sex, 2022\"\n",
    "    table = soup.find(\"caption\", string=lambda text: caption_text in text if text else False).find_parent('table') if soup.find(\"caption\", string=lambda text: caption_text in text if text else False) else None\n",
    "\n",
    "    if table:\n",
    "        # Extract headers from the table's thead element\n",
    "        headers = [th.get_text(strip=True) for th in table.find('thead').find_all('th')[1:]]  # Skip the first empty <th>\n",
    "        headers.insert(0, \"Age Group\")  # Manually add \"Age Group\" as the first header\n",
    "\n",
    "        # Collecting data rows\n",
    "        data = []\n",
    "        for row in table.find('tbody').find_all('tr'):\n",
    "            cells = [td.get_text(strip=True) for td in row.find_all('td')]\n",
    "            age_group = row.find('th').get_text(strip=True) if row.find('th') else \"\"\n",
    "            cells.insert(0, age_group)  # Insert the age group at the start of the list of cells\n",
    "\n",
    "            if len(cells) == len(headers):  # Ensure the row has the correct number of elements\n",
    "                data.append(cells)\n",
    "\n",
    "        # Creating a Spark DataFrame\n",
    "        df_age_sex_group = spark.createDataFrame(data, schema=headers)\n",
    "\n",
    "        # Convert relevant columns to float\n",
    "        for col_name in ['Males (%)', 'Females (%)']:\n",
    "            df_age_sex_group = df_age_sex_group.withColumn(col_name, col(col_name).cast('float'))\n",
    "\n",
    "        # Select only the age group and the converted columns\n",
    "        df_age_sex_group = df_age_sex_group.select('Age Group', 'Males (%)', 'Females (%)')\n",
    "\n",
    "        df_age_sex_group.show(8)\n",
    "    else:\n",
    "        print(\"Table not found under this caption.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve data, status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec02d95-a0a2-48e9-a3c8-650092870a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the function to categorize age\n",
    "def categorize_age(age):\n",
    "    if age < 18:\n",
    "        return \"15â€“17\"\n",
    "    elif age < 25:\n",
    "        return \"18â€“24\"\n",
    "    elif age < 35:\n",
    "        return \"25â€“34\"\n",
    "    elif age < 45:\n",
    "        return \"35â€“44\"\n",
    "    elif age < 55:\n",
    "        return \"45â€“54\"\n",
    "    elif age < 65:\n",
    "        return \"55â€“64\"\n",
    "    elif age < 75:\n",
    "        return \"65â€“74\"\n",
    "    else:\n",
    "        return \"75 years and over\"\n",
    "\n",
    "# Convert the function to a UDF\n",
    "categorize_age_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column 'Age Group'\n",
    "df_imputed = df_imputed.withColumn('Age Group', categorize_age_udf(col('age')))\n",
    "\n",
    "# Show the result\n",
    "df_imputed.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908b5c7-4927-490a-aa4b-5f92d9c35f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "# Broadcast the data required for the UDF\n",
    "age_group_broadcast = sc.broadcast({row['Age Group']: row['Average Smoking Rate (%)'] for row in df_age_group.collect()})\n",
    "age_sex_group_broadcast = sc.broadcast({row['Age Group']: (row['Males (%)'], row['Females (%)']) for row in df_age_sex_group.collect()})\n",
    "\n",
    "# Define the impute_smoking function\n",
    "def impute_smoking(age_group, gender):\n",
    "    age_group_dict = age_group_broadcast.value\n",
    "    age_sex_group_dict = age_sex_group_broadcast.value\n",
    "    \n",
    "    if gender == 1:  # Male\n",
    "        if age_group in age_sex_group_dict:\n",
    "            age_rate = age_group_dict.get(age_group, None)\n",
    "            male_rate, female_rate = age_sex_group_dict[age_group]\n",
    "            rate = (age_rate * male_rate) / female_rate if female_rate != 0 else male_rate\n",
    "        else:\n",
    "            rate = None \n",
    "    else:  # Female\n",
    "        if age_group in age_group_dict:\n",
    "            rate = age_group_dict.get(age_group, None)\n",
    "        else:\n",
    "            rate = None  \n",
    "\n",
    "    return rate\n",
    "\n",
    "# Convert the function to a UDF\n",
    "impute_smoking_udf = udf(impute_smoking, FloatType())\n",
    "\n",
    "# Assuming df_imputed is your main DataFrame with 'Age Group' and 'sex' columns\n",
    "\n",
    "# Define the function to categorize age\n",
    "def categorize_age(age):\n",
    "    if age < 18:\n",
    "        return \"15â€“17\"\n",
    "    elif age < 25:\n",
    "        return \"18â€“24\"\n",
    "    elif age < 35:\n",
    "        return \"25â€“34\"\n",
    "    elif age < 45:\n",
    "        return \"35â€“44\"\n",
    "    elif age < 55:\n",
    "        return \"45â€“54\"\n",
    "    elif age < 65:\n",
    "        return \"55â€“64\"\n",
    "    elif age < 75:\n",
    "        return \"65â€“74\"\n",
    "    else:\n",
    "        return \"75 years and over\"\n",
    "\n",
    "# Convert the function to a UDF\n",
    "categorize_age_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Load your actual main DataFrame\n",
    "# df_imputed = spark.read.csv('path_to_your_actual_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Apply the UDF to create a new column 'Age Group'\n",
    "df_imputed = df_imputed.withColumn('Age Group', categorize_age_udf(col('age')))\n",
    "\n",
    "# Apply the UDF to create a new column 'smoke'\n",
    "df_imputed = df_imputed.withColumn('smoke', impute_smoking_udf(col('Age Group'), col('sex')))\n",
    "\n",
    "# Round and convert the 'smoke' column to integer\n",
    "df_imputed = df_imputed.withColumn('smoke', col('smoke').cast('int'))\n",
    "\n",
    "# Show the result\n",
    "df_imputed.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2551f-7a84-45f4-875e-9afed0e6e115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98600fdd-dca3-466c-9357-11c70350a538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80435228-28fe-4e17-b180-5c283b0bfc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3802870-69f1-4886-9ec7-4e64782d934b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d672510-217c-43e0-85b8-10ddfef4b30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

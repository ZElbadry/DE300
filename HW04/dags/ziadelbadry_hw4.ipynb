{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e073ae1-33a4-46a0-b998-888767621b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator, BranchPythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import tomli\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuration parameters\n",
    "CONFIG_BUCKET = \"de300-ziadelbadry\"\n",
    "CONFIG_FILE = \"hw4_config.toml\"\n",
    "\n",
    "TABLE_NAMES = {\n",
    "    \"original_data\": \"original_data\",\n",
    "    \"cleaned_data_stage1\": \"cleaned_data_s1\",\n",
    "    \"cleaned_data_stage2\": \"cleaned_data_s2\",\n",
    "    \"training_data_stage1\": \"training_data_s1\",\n",
    "    \"training_data_stage2\": \"training_data_s2\",\n",
    "    \"test_data_stage1\": \"test_data_s1\",\n",
    "    \"test_data_stage2\": \"test_data_s2\",\n",
    "    \"normalized_data_stage1\": \"normalized_data_s1\",\n",
    "    \"normalized_data_stage2\": \"normalized_data_s2\",\n",
    "    \"high_risk_features_stage1\": \"high_risk_features_s1\",\n",
    "    \"product_features_stage1\": \"product_features_s1\",\n",
    "    \"high_risk_features_stage2\": \"high_risk_features_s2\",\n",
    "    \"product_features_stage2\": \"product_features_s2\",\n",
    "    \"gender_based_statistics\": \"gender_stats\",\n",
    "    \"age_based_statistics_stage1\": \"age_stats_s1\",\n",
    "    \"age_based_statistics_stage2\": \"age_stats_s2\",\n",
    "    \"smoke_scrape_merged\": \"smoke_data_merged\"\n",
    "}\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ziadelbadry',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 2,\n",
    "}\n",
    "\n",
    "# DAG definition\n",
    "dag = DAG(\n",
    "    'Ziad',\n",
    "    default_args=default_args,\n",
    "    description='Heart Disease Data Pipeline',\n",
    "    schedule_interval='@daily',\n",
    "    tags=[\"Heart Disease\", \"Data Processing\", \"Machine Learning\"]\n",
    ")\n",
    "\n",
    "# Read config from S3\n",
    "def read_config_from_s3() -> dict:\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=CONFIG_BUCKET, Key=CONFIG_FILE)\n",
    "        file_content = response['Body'].read()\n",
    "        params = tomli.loads(file_content.decode('utf-8'))\n",
    "        return params\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read from S3: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "PARAMS = read_config_from_s3()\n",
    "\n",
    "# Database connection\n",
    "def create_db_connection():\n",
    "    conn_uri = f\"{PARAMS['db']['db_alchemy_driver']}://{PARAMS['db']['username']}:{PARAMS['db']['password']}@{PARAMS['db']['host']}:{PARAMS['db']['port']}/{PARAMS['db']['db_name']}\"\n",
    "    engine = create_engine(conn_uri)\n",
    "    connection = engine.connect()\n",
    "    return connection\n",
    "\n",
    "# Decorator for managing data flow between tasks\n",
    "def from_table_to_df(input_table_names: list[str], output_table_names: list[str]):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            conn = create_db_connection()\n",
    "            dfs = [pd.read_sql(f\"SELECT * FROM {name}\", conn) for name in input_table_names]\n",
    "            kwargs['dfs'] = dfs[0] if len(input_table_names) == 1 else dfs\n",
    "            result = func(*args, **kwargs)\n",
    "            for name in output_table_names:\n",
    "                conn.execute(f\"DROP TABLE IF EXISTS {name}\")\n",
    "            for pair in result['dfs']:\n",
    "                pair['df'].to_sql(pair['table_name'], conn, if_exists=\"replace\", index=False)\n",
    "            conn.close()\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Task Definitions\n",
    "\n",
    "# Initialize database schema\n",
    "initialize_schema = PostgresOperator(\n",
    "    task_id=\"initialize_schema\",\n",
    "    postgres_conn_id=PARAMS['db']['db_connection'],\n",
    "    sql=f\"\"\"\n",
    "    DROP SCHEMA public CASCADE;\n",
    "    CREATE SCHEMA public;\n",
    "    GRANT ALL ON SCHEMA public TO {PARAMS['db']['username']};\n",
    "    GRANT ALL ON SCHEMA public TO public;\n",
    "    COMMENT ON SCHEMA public IS 'standard public schema';\n",
    "    \"\"\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Data ingestion from external source\n",
    "ingest_data = PythonOperator(\n",
    "    task_id='ingest_data',\n",
    "    python_callable=add_data_to_table_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Data cleaning and preparation\n",
    "prepare_data_stage1 = PythonOperator(\n",
    "    task_id='prepare_data_s1',\n",
    "    python_callable=clean_and_impute_data_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "prepare_data_stage2 = PythonOperator(\n",
    "    task_id='prepare_data_s2',\n",
    "    python_callable=clean_and_impute_data_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Data normalization\n",
    "normalize_data_s1 = PythonOperator(\n",
    "    task_id='normalize_data_s1',\n",
    "    python_callable=normalize_data_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "normalize_data_s2 = PythonOperator(\n",
    "    task_id='normalize_data_s2',\n",
    "    python_callable=normalize_data_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Exploratory data analysis\n",
    "perform_eda_s1 = PythonOperator(\n",
    "    task_id='perform_eda_s1',\n",
    "    python_callable=eda_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "perform_eda_s2 = PythonOperator(\n",
    "    task_id='perform_eda_s2',\n",
    "    python_callable=eda_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Feature engineering\n",
    "engineer_features_high_risk_s1 = PythonOperator(\n",
    "    task_id='engineer_features_high_risk_s1',\n",
    "    python_callable=fe_high_risk_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "engineer_features_product_s1 = PythonOperator(\n",
    "    task_id='engineer_features_product_s1',\n",
    "    python_callable=fe_product_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "engineer_features_high_risk_s2 = PythonOperator(\n",
    "    task_id='engineer_features_high_risk_s2',\n",
    "    python_callable=fe_high_risk_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "engineer_features_product_s2 = PythonOperator(\n",
    "    task_id='engineer_features_product_s2',\n",
    "    python_callable=fe_product_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Model training\n",
    "train_logistic_regression_product_s1 = PythonOperator(\n",
    "    task_id='train_logistic_regression_product_s1',\n",
    "    python_callable=product_lr_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_logistic_regression_high_risk_s1 = PythonOperator(\n",
    "    task_id='train_logistic_regression_high_risk_s1',\n",
    "    python_callable=high_risk_lr_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_production_model_s1 = PythonOperator(\n",
    "    task_id='train_production_model_s1',\n",
    "    python_callable=production_lr_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_support_vector_machine_product_s1 = PythonOperator(\n",
    "    task_id='train_support_vector_machine_product_s1',\n",
    "    python_callable=product_svm_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_support_vector_machine_high_risk_s1 = PythonOperator(\n",
    "    task_id='train_support_vector_machine_high_risk_s1',\n",
    "    python_callable=high_risk_svm_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_production_model_svm_s1 = PythonOperator(\n",
    "    task_id='train_production_model_svm_s1',\n",
    "    python_callable=production_svm_1_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_logistic_regression_product_s2 = PythonOperator(\n",
    "    task_id='train_logistic_regression_product_s2',\n",
    "    python_callable=product_lr_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_logistic_regression_high_risk_s2 = PythonOperator(\n",
    "    task_id='train_logistic_regression_high_risk_s2',\n",
    "    python_callable=high_risk_lr_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_production_model_s2 = PythonOperator(\n",
    "    task_id='train_production_model_s2',\n",
    "    python_callable=production_lr_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_support_vector_machine_product_s2 = PythonOperator(\n",
    "    task_id='train_support_vector_machine_product_s2',\n",
    "    python_callable=product_svm_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_support_vector_machine_high_risk_s2 = PythonOperator(\n",
    "    task_id='train_support_vector_machine_high_risk_s2',\n",
    "    python_callable=high_risk_svm_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "train_production_model_svm_s2 = PythonOperator(\n",
    "    task_id='train_production_model_svm_s2',\n",
    "    python_callable=production_svm_2_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Model selection based on performance\n",
    "select_best_model = BranchPythonOperator(\n",
    "    task_id='select_best_model',\n",
    "    python_callable=decide_which_model,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Placeholder for no action\n",
    "no_action = DummyOperator(\n",
    "    task_id='no_action',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Data scraping for additional insights\n",
    "scrape_and_prepare_smoke_data = PythonOperator(\n",
    "    task_id='scrape_and_prepare_smoke_data',\n",
    "    python_callable=scrape_smoke_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Combine scraped data with existing data\n",
    "combine_scraped_data = PythonOperator(\n",
    "    task_id='combine_scraped_data',\n",
    "    python_callable=merge_smoke_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Model performance evaluation after merging new data\n",
    "evaluate_merged_logistic_regression = PythonOperator(\n",
    "    task_id='evaluate_merged_logistic_regression',\n",
    "    python_callable=merge_lr_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "evaluate_merged_svm = PythonOperator(\n",
    "    task_id='evaluate_merged_svm',\n",
    "    python_callable=merge_svm_func,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define additional evaluation tasks dynamically based on feature types\n",
    "evaluation_tasks = []\n",
    "for feature_type in feature_operations:\n",
    "    task_id = f\"evaluate_{feature_type}\"\n",
    "    evaluation_tasks.append(PythonOperator(\n",
    "        task_id=task_id,\n",
    "        python_callable=locals()[f\"{task_id}_func\"],\n",
    "        provide_context=True,\n",
    "        dag=dag\n",
    "    ))\n",
    "\n",
    "# Task dependencies\n",
    "initialize_schema >> ingest_data >> [prepare_data_stage1, prepare_data_stage2]\n",
    "prepare_data_stage1 >> [normalize_data_s1, perform_eda_s1]\n",
    "prepare_data_stage2 >> [normalize_data_s2, perform_eda_s2]\n",
    "normalize_data_s1 >> [engineer_features_high_risk_s1, engineer_features_product_s1] >> [train_logistic_regression_high_risk_s1, train_logistic_regression_product_s1, train_support_vector_machine_high_risk_s1, train_support_vector_machine_product_s1] >> select_best_model\n",
    "normalize_data_s2 >> [engineer_features_high_risk_s2, engineer_features_product_s2] >> [train_logistic_regression_high_risk_s2, train_logistic_regression_product_s2, train_support_vector_machine_high_risk_s2, train_support_vector_machine_product_s2] >> select_best_model\n",
    "[engineer_features_high_risk_s1, engineer_features_product_s2, scrape_and_prepare_smoke_data] >> combine_scraped_data >> [evaluate_merged_logistic_regression, evaluate_merged_svm] >> select_best_model\n",
    "[select_best_model] >> [no_action, *evaluation_tasks]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

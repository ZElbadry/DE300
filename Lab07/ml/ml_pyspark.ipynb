{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e7f5c5-1f5e-4773-8950-66be75241122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from itertools import combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48ca542-50c6-4062-8906-47d6ae440bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, pow\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.param.shared import Param, Params\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from itertools import combinations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a5270-37a8-4a0e-9663-2f0c09b88269",
   "metadata": {},
   "source": [
    "## Check Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06b828d-1d7a-41a3-8929-703028401ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/demos/bin/python3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12fef7f-59b8-4b2b-b7b6-f2fff328616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "NUMBER_OF_FOLDS = 3\n",
    "SPLIT_SEED = 7576\n",
    "TRAIN_TEST_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a794b4a-3efa-48bc-a8c3-2df602c90670",
   "metadata": {},
   "source": [
    "## Function for data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ab6c2d-df8f-4aaf-bdfe-a6f30efaba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    read data; since the data has the header we let spark guess the schema\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the Titanic CSV data into a DataFrame\n",
    "    titanic_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(os.path.join(DATA_FOLDER,\"*.csv\"))\n",
    "\n",
    "    return titanic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6afbb-f7ce-4f7a-897b-070bfb496efe",
   "metadata": {},
   "source": [
    "## Writing new Transformer type class : adding cross product of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cef9e877-a67b-4757-96d6-571c8fc02579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseProduct(Transformer):\n",
    "\n",
    "    def __init__(self, inputCols, outputCols):\n",
    "        self.__inputCols = inputCols\n",
    "        self.__outputCols = outputCols\n",
    "\n",
    "        self._paramMap = self._params = {}\n",
    "\n",
    "    def _transform(self, df):\n",
    "        for cols, out_col in zip(self.__inputCols, self.__outputCols):\n",
    "            df = df.withColumn(out_col, col(cols[0]) * col(cols[1]))\n",
    "        return df\n",
    "\n",
    "class PairwiseProductAndSquare(Transformer, Params):\n",
    "    inputCols = Param(Params._dummy(), \"inputCols\", \"Input columns\")\n",
    "\n",
    "    def __init__(self, inputCols=None):\n",
    "        super(PairwiseProductAndSquare, self).__init__()\n",
    "        self._setDefault(inputCols=None)\n",
    "        self._set(inputCols=inputCols)\n",
    "\n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        input_cols = self.getOrDefault(self.inputCols)\n",
    "        \n",
    "        for i, col1 in enumerate(input_cols):\n",
    "            for col2 in input_cols[i:]:\n",
    "                new_col_name = f\"{col1}*{col2}\"\n",
    "                df = df.withColumn(new_col_name, df[col1] * df[col2])\n",
    "                \n",
    "            squared_col_name = f\"{col1}^2\"\n",
    "            df = df.withColumn(squared_col_name, pow(df[col1], 2))\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da37d86-c3aa-43c2-a838-7b072140259e",
   "metadata": {},
   "source": [
    "## The ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e5c76c-e845-4bfa-a311-cc98a03cfcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data: DataFrame):\n",
    "\n",
    "    \"\"\"\n",
    "    every attribute that is numeric is non-categorical; this is questionable\n",
    "    \"\"\"\n",
    "\n",
    "    numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, DoubleType) or isinstance(f.dataType, FloatType) or isinstance(f.dataType, IntegerType) or isinstance(f.dataType, LongType)]\n",
    "    string_features = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    numeric_features.remove(\"PassengerId\")\n",
    "    numeric_features.remove(\"Survived\")\n",
    "    string_features.remove(\"Name\")\n",
    "\n",
    "    # index string features; map string to consecutive integers - it should be one hot encoding \n",
    "    name_indexed_string_columns = [f\"{v}Index\" for v in string_features] \n",
    "    # we must have keep so that we can impute them in the next step\n",
    "    indexer = StringIndexer(inputCols=string_features, outputCols=name_indexed_string_columns, handleInvalid='keep')\n",
    "\n",
    "    # Fill missing values; strategy can be mode, median, mean\n",
    "    \n",
    "    # string columns\n",
    "    imputed_columns_string = [f\"Imputed{v}\" for v in name_indexed_string_columns]\n",
    "    imputers_string = []\n",
    "    for org_col_name, indexed_col_name, imputed_col_name in zip(string_features, name_indexed_string_columns, imputed_columns_string):\n",
    "        # Count the number of distinct categories in the original column\n",
    "        number_of_categories = data.select(F.countDistinct(org_col_name)).take(1)[0].asDict()[f'count(DISTINCT {org_col_name})']\n",
    "        \n",
    "        # Create an imputer for the indexed column\n",
    "        # this is the value that needs to be imputed based on the keep option above\n",
    "        imputer = Imputer(inputCol=indexed_col_name, outputCol=imputed_col_name, strategy = \"mode\", missingValue=number_of_categories)\n",
    "\n",
    "        # Append the imputer to the list\n",
    "        imputers_string.append(imputer)\n",
    "\n",
    "    \n",
    "    # numeric columns\n",
    "    imputed_columns_numeric = [f\"Imputed{v}\" for v in numeric_features]\n",
    "    imputer_numeric = Imputer(inputCols=numeric_features, outputCols=imputed_columns_numeric, strategy = \"mean\")\n",
    "\n",
    "    # Create all pairwise products of numeric features\n",
    "    all_pairs = [v for v in combinations(imputed_columns_numeric, 2)]\n",
    "    pairwise_columns = [f\"{col1}_{col2}\" for col1, col2 in all_pairs]\n",
    "    pairwise_product = PairwiseProduct(inputCols=all_pairs, outputCols=pairwise_columns)\n",
    "\n",
    "    # Assemble feature columns into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=pairwise_columns + imputed_columns_numeric + imputed_columns_string, \n",
    "        outputCol=\"features\"\n",
    "        )\n",
    "\n",
    "    # Define a Random Forest classifier\n",
    "    classifier = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(stages=[indexer, *imputers_string, imputer_numeric, pairwise_product, assembler, classifier])\n",
    "    \n",
    "    # Set up the parameter grid for maximum tree depth\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classifier.maxDepth, [2, 4, 6, 8, 10]) \\\n",
    "        .build()\n",
    "\n",
    "    # Set up the cross-validator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=NUMBER_OF_FOLDS,\n",
    "        seed=SPLIT_SEED)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = data.randomSplit([TRAIN_TEST_SPLIT, 1-TRAIN_TEST_SPLIT], seed=SPLIT_SEED)\n",
    "\n",
    "    # Train the cross-validated pipeline model\n",
    "    cvModel = crossval.fit(train_data)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = cvModel.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    sys.stdout.write(f\"Area Under ROC Curve: {auc:.4f}\\n\")\n",
    "\n",
    "    # Get the best RandomForest model\n",
    "    best_model = cvModel.bestModel.stages[-1]\n",
    "\n",
    "    # Retrieve the selected maximum tree depth\n",
    "    selected_max_depth = best_model.getOrDefault(best_model.getParam(\"maxDepth\"))\n",
    "\n",
    "    # Print the selected maximum tree depth\n",
    "    sys.stdout.write(f\"Selected Maximum Tree Depth: {selected_max_depth}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a6c908-cfc7-431e-a19e-080b41efb258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_pipeline(data: DataFrame):\n",
    "    numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, DoubleType) or isinstance(f.dataType, FloatType) or isinstance(f.dataType, IntegerType) or isinstance(f.dataType, LongType)]\n",
    "    string_features = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    numeric_features.remove(\"PassengerId\")\n",
    "    numeric_features.remove(\"Survived\")\n",
    "    string_features.remove(\"Name\")\n",
    "\n",
    "    # Index string features; map string to consecutive integers - it should be one hot encoding \n",
    "    name_indexed_string_columns = [f\"{v}Index\" for v in string_features]\n",
    "    indexer = StringIndexer(inputCols=string_features, outputCols=name_indexed_string_columns, handleInvalid='keep')\n",
    "\n",
    "    # Fill missing values; strategy can be mode, median, mean\n",
    "    imputed_columns_string = [f\"Imputed{v}\" for v in name_indexed_string_columns]\n",
    "    imputers_string = []\n",
    "    for org_col_name, indexed_col_name, imputed_col_name in zip(string_features, name_indexed_string_columns, imputed_columns_string):\n",
    "        number_of_categories = data.select(F.countDistinct(org_col_name)).take(1)[0].asDict()[f'count(DISTINCT {org_col_name})']\n",
    "        imputer = Imputer(inputCol=indexed_col_name, outputCol=imputed_col_name, strategy=\"mode\", missingValue=number_of_categories)\n",
    "        imputers_string.append(imputer)\n",
    "\n",
    "    # Numeric columns\n",
    "    imputed_columns_numeric = [f\"Imputed{v}\" for v in numeric_features]\n",
    "    imputer_numeric = Imputer(inputCols=numeric_features, outputCols=imputed_columns_numeric, strategy=\"mean\")\n",
    "\n",
    "    # Create all pairwise products of numeric features\n",
    "    pairwise_transformer = PairwiseProductAndSquare(inputCols=imputed_columns_numeric)\n",
    "\n",
    "    # Assemble feature columns into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputed_columns_numeric + imputed_columns_string, \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Define a Random Forest classifier\n",
    "    classifier = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(stages=[indexer, *imputers_string, imputer_numeric, pairwise_transformer, assembler, classifier])\n",
    "    \n",
    "    # Set up the parameter grid for maximum tree depth and number of trees\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(classifier.maxDepth, [2, 4, 6, 8, 10]) \\\n",
    "        .addGrid(classifier.numTrees, [20, 50, 100]) \\\n",
    "        .build()\n",
    "\n",
    "    # Set up the cross-validator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "    # Train the cross-validated pipeline model\n",
    "    cvModel = crossval.fit(train_data)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = cvModel.transform(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    sys.stdout.write(f\"Modified - Area Under ROC Curve: {auc:.4f}\\n\")\n",
    "\n",
    "    # Get the best RandomForest model\n",
    "    best_model = cvModel.bestModel.stages[-1]\n",
    "\n",
    "    # Retrieve the selected maximum tree depth\n",
    "    selected_max_depth = best_model.getOrDefault(best_model.getParam(\"maxDepth\"))\n",
    "\n",
    "    # Print the selected maximum tree depth\n",
    "    sys.stdout.write(f\"Modified - Selected Maximum Tree Depth: {selected_max_depth}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315b3402-ad5f-4e46-a317-e0c3d804963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/23 01:47:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC Curve: 0.8661\n",
      "Selected Maximum Tree Depth: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 01:50:48 WARN DAGScheduler: Broadcasting large task binary with size 1145.9 KiB\n",
      "24/05/23 01:51:02 WARN DAGScheduler: Broadcasting large task binary with size 1145.9 KiB\n",
      "24/05/23 01:51:02 WARN DAGScheduler: Broadcasting large task binary with size 1371.3 KiB\n",
      "24/05/23 01:51:02 WARN DAGScheduler: Broadcasting large task binary with size 1563.4 KiB\n",
      "24/05/23 01:51:03 WARN DAGScheduler: Broadcasting large task binary with size 1103.6 KiB\n",
      "24/05/23 01:51:51 WARN DAGScheduler: Broadcasting large task binary with size 1088.2 KiB\n",
      "24/05/23 01:52:04 WARN DAGScheduler: Broadcasting large task binary with size 1088.2 KiB\n",
      "24/05/23 01:52:04 WARN DAGScheduler: Broadcasting large task binary with size 1291.0 KiB\n",
      "24/05/23 01:52:05 WARN DAGScheduler: Broadcasting large task binary with size 1461.1 KiB\n",
      "24/05/23 01:52:06 WARN DAGScheduler: Broadcasting large task binary with size 1073.0 KiB\n",
      "24/05/23 01:52:55 WARN DAGScheduler: Broadcasting large task binary with size 1076.5 KiB\n",
      "24/05/23 01:53:07 WARN DAGScheduler: Broadcasting large task binary with size 1076.5 KiB\n",
      "24/05/23 01:53:08 WARN DAGScheduler: Broadcasting large task binary with size 1271.3 KiB\n",
      "24/05/23 01:53:08 WARN DAGScheduler: Broadcasting large task binary with size 1433.9 KiB\n",
      "24/05/23 01:53:09 WARN DAGScheduler: Broadcasting large task binary with size 1038.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified - Area Under ROC Curve: 0.9019\n",
      "Modified - Selected Maximum Tree Depth: 4\\m"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Predict Titanic Survival\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    data = read_data(spark)\n",
    "    \n",
    "    pipeline(data)\n",
    "    modified_pipeline(data)\n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e651daa-94b9-4aa9-a091-90950d9450a1",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Results:\n",
    "\n",
    "1. **Original Pipeline:**\n",
    "   - **Area Under ROC Curve (AUC):** 0.8812\n",
    "   - **Selected Maximum Tree Depth:** 4\n",
    "\n",
    "2. **Modified Pipeline with PairwiseProductAndSquare:**\n",
    "   - **Area Under ROC Curve (AUC):** 0.8997\n",
    "   - **Selected Maximum Tree Depth:** 4\n",
    "\n",
    "### Improvement in AUC:\n",
    "- The AUC improved from 0.8812 to 0.8997 with the introduction of the `PairwiseProductAndSquare` transformer. This indicates that the model's ability to distinguish between the positive and negative classes has improved with the additional pairwise product and squared features.\n",
    "\n",
    "### Stability in Tree Depth:\n",
    "- The selected maximum tree depth remained at 4 for both the original and modified pipelines. This suggests that the additional features introduced by the `PairwiseProductAndSquare` transformer have provided more discriminative power without increasing the complexity of the model.\n",
    "\n",
    "### Conclusion:\n",
    "The introduction of the `PairwiseProductAndSquare` transformer has led to a noticeable improvement in model performance, as evidenced by the higher AUC. The fact that the maximum tree depth remains unchanged indicates that the model complexity has not increased, yet its predictive power has enhanced. These results suggest that the additional features generated by the transformer are beneficial in capturing important interactions between the numeric features, thus improving the model's predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
